[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "llm_memory_estimator"
version = "0.1.0"
description = "Estimate GPU memory requirements for LLM/VLM fine-tuning"
readme = "README.md"
requires-python = ">=3.8"
license = {text = "MIT"}
authors = [
    {name = "LLM Memory Estimator Contributors"}
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "transformers>=4.36.0",
    "accelerate>=0.25.0",
    "torch>=2.1.0",
    "pandas>=2.0.0",
    "gradio>=4.40.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
]
cuda-full = [
    "peft>=0.8.0",
    "bitsandbytes>=0.43.0",
    "flash-attn>=2.5.0",
    "deepspeed>=0.14.0",
    "liger-kernel>=0.1.0",
]

[project.scripts]
llm-memory-estimator = "llm_memory_estimator.cli:main"

[project.urls]
Homepage = "https://github.com/llm-memory-estimator/llm_memory_estimator"
Repository = "https://github.com/llm-memory-estimator/llm_memory_estimator"

# uv configuration: CPU-only PyTorch by default (saves ~1GB vs CUDA version)
# For CUDA support with probe:
#   1. Remove torch from dependencies or install cuda-full extras
#   2. Install CUDA torch manually: pip install torch --index-url https://download.pytorch.org/whl/cu121
#   3. Then: uv sync --extra cuda-full
[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[tool.uv.sources]
torch = [
    { index = "pytorch-cpu" }
]

[tool.hatchling.build.targets.wheel]
packages = ["llm_memory_estimator"]

[tool.hatchling.build.targets.wheel.shared-data]
"*.md" = "llm_memory_estimator"
"*.json" = "llm_memory_estimator"

[tool.black]
line-length = 100
target-version = ["py38", "py39", "py310", "py311"]

[tool.ruff]
line-length = 100
target-version = "py38"

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
]
ignore = [
    "E501", # line too long (handled by black)
    "B008", # do not perform function calls in argument defaults
]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
